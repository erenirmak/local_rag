{
    "summary": "This code initializes NLP models, sets up LangChain chain with Zephyr LLM and RetrievalQA, connects to PostgreSQL, and creates a function for chat execution. It checks if the input is \"exit\" and terminates if true, otherwise runs 'rag_chain_run' with given query.",
    "details": [
        {
            "comment": "The code is loading a model and pipeline for natural language processing, retrieving and processing documents, getting embeddings from a database, initializing a RAG chain for retrieval-based QA tasks, logging into HuggingFace, and loading a specific model from the HuggingFace repository. The code also specifies an API token and a model name with a revision.",
            "location": "\"/media/root/Prima/works/local_rag/docs/src/Theasy.py\":1-32",
            "content": "import os # file system\nfrom huggingface_hub import login\n# get model & pipeline\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\nfrom langchain.llms.huggingface_pipeline import HuggingFacePipeline\n# load & process documents\nfrom langchain.document_loaders import PyPDFLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n# get embeddings\nfrom langchain.embeddings import HuggingFaceEmbeddings\n# save embeddings to vector database\nfrom langchain.vectorstores.pgvector import PGVector\n# RAG chain\nfrom langchain.chains import RetrievalQA\n# login Huggingface\nHUGGINGFACEHUB_API_TOKEN = \"YOUR_HUGGINGFACE_API_TOKEN\"\nlogin(HUGGINGFACEHUB_API_TOKEN)\n# Load Model from Huggingface\nmodel_name = \"TheBloke/zephyr-7B-beta-GPTQ\"\n# To use a different branch, change revision\n# For example: revision=\"gptq-4bit-32g-actorder_True\"\nmodel = AutoModelForCausalLM.from_pretrained(model_name,\n                                             device_map=\"auto\",\n                                             trust_remote_code=False,"
        },
        {
            "comment": "The code initializes a text generation pipeline using the Hugging Face library. It defines a LangChain LLM based on this pipeline, loads PDF files from a specified directory, and stores them in a list of lists containing Document objects.",
            "location": "\"/media/root/Prima/works/local_rag/docs/src/Theasy.py\":33-63",
            "content": "                                             revision=\"main\")\ntokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n# Create Pipeline\npipe = pipeline(\n        \"text-generation\",\n        model=model,\n        tokenizer=tokenizer,\n        use_cache=True,\n        device_map=\"auto\",\n        max_length=5000,\n        do_sample=True,\n        top_k=5,\n        num_return_sequences=1,\n        eos_token_id=tokenizer.eos_token_id,\n        pad_token_id=tokenizer.eos_token_id,\n)\n# LangChain LLM definition based on pipeline\nllm = HuggingFacePipeline(pipeline=pipe) # [tokenizer -> model] linked\n# Load PDF Files\npdf_path = \"articles\" # change the name of the folder where your documents lies\nfile_list = os.listdir(pdf_path)\nos.chdir(pdf_path)\nall_contents = []\nfor file in file_list:\n    file_loader = PyPDFLoader(file)\n    file_contents = file_loader.load() # returns splitted \"list\" of Document objects\n    all_contents.append(file_contents) # list that contains list of Document objects: [[Document, Document..], [Document, Document..], [Document, Document..]..]"
        },
        {
            "comment": "Rearranges Document objects, splits documents into chunks using RecursiveCharacterTextSplitter, sets up sentence transformers embeddings and initializes VectorDB with a PostgreSQL connection string.",
            "location": "\"/media/root/Prima/works/local_rag/docs/src/Theasy.py\":65-88",
            "content": "# Rearrange Document objects\ndocuments = []\nfor doc_list in all_contents:\n    if type(doc_list) == list:\n        for doc in doc_list:\n            documents.append(doc)\n    else:\n        documents.append(doc_list)\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=250, chunk_overlap=20)\nall_splits = text_splitter.split_documents(documents)\n# Embedding setup\nmodel_name = \"sentence-transformers/all-mpnet-base-v2\"\nmodel_kwargs = {\"device\": \"cuda\"}\nembeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs) # sentence transformer embeddings\n# Vectordb Setup\n## Generate connection string\nconnection_string = PGVector.connection_string_from_db_params(driver=\"YOUR DRIVER\", # I used psycopg2, you can use another\n                                                              user=\"YOUR USER\",\n                                                              password=\"YOUR PASS\",\n                                                              host=\"localhost\",\n                                                              port=\"5432\","
        },
        {
            "comment": "This code is setting up a language model chain using the LangChain library. It uses a Zephyr model as the LLM, and the RetrievalQA chain type with refine, map_reduce, and map_rerank functionalities. The code also includes a function `rag_chain_run` for executing the chain and retrieves results based on user input queries in a chat interface.",
            "location": "\"/media/root/Prima/works/local_rag/docs/src/Theasy.py\":89-116",
            "content": "                                                              database=\"YOUR DATABASE\")\nvectordb = PGVector.from_documents(documents = all_splits,\n                                   embedding = embeddings,\n                                   collection_name = \"articles\",\n                                   connection_string = connection_string,\n                                   pre_delete_collection = True)\nretriever = vectordb.as_retriever() # sentence transformer embeddings -> vectordb -> retriever (converted)\n#######################\nqa_chain = RetrievalQA.from_chain_type(\n    llm=llm, # zephyr model\n    chain_type=\"stuff\", # (refine, map_reduce, map_rerank) see here: https://python.langchain.com/docs/modules/chains/document/\n    retriever=retriever, # sentence transformer in the background\n    verbose=True\n)\ndef rag_chain_run(qa, query):\n    result = qa.run(query)\n    print(\"\\nResult: \", result)\nprint(\"Welcome to chat interface! Please type your query:\")\n# RAG experiment:\nwhile 1:\n    query = input(\"Custom query: \")"
        },
        {
            "comment": "This code snippet checks if the input query is \"exit\" and terminates the loop if it is. Otherwise, it calls the function 'rag_chain_run' with the existing qa_chain and the current query as arguments.",
            "location": "\"/media/root/Prima/works/local_rag/docs/src/Theasy.py\":117-120",
            "content": "    if query == \"exit\":\n        break\n    rag_chain_run(qa_chain, query)"
        }
    ]
}