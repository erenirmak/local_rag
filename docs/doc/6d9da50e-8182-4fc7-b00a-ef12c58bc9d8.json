{
    "summary": "This code sets up a local system for chatting with multiple PDF files using Zephyr-7b-beta, addressing hallucination in LLMs via RAG algorithm. It suggests using PDFs for analysis tasks and encourages users to design UI or dockerize the application.",
    "details": [
        {
            "comment": "This code is for setting up a local system to chat with multiple PDF files using a language model, utilizing PostgreSQL database and pgvector extension. It also lists the required Python packages and dependencies.",
            "location": "\"/media/root/Prima/works/local_rag/docs/src/README.md\":0-54",
            "content": "# Chat with Your Multiple PDFs on Your Local System\nUsing a language model, you can talk with your PDF files. This also lets your language model\ndo less mistakes and hallucinations.\n## Setup Database\nI used PostgreSQL in this project because it is lightweight and robust.\nYou can download it from here: https://www.postgresql.org/download/\n## Setup Vector Extension\nPostgreSQL has an extension: pgvector. You can use it to store embeddings.\nYou can download it from here: https://github.com/pgvector/pgvector\nFollow the instructions to install the extension.\nThen, install the python package:\n```\npip3 install pgvector\n```\n## Requirements\n<p> datasets==2.15.0\n<p> huggingface_hub==0.19.4\n<p> langchain==0.0.341\n<p> pgvector==0.2.4\n<p> psycopg2==2.9.9\n<p> SQLAlchemy==2.0.23\n<p> torch==2.1.1+cu118\n<p> transformers==4.35.2\n<p> bitsandbytes==0.41.2.post2 (bitsandbytes-windows)\n<p> peft==0.6.2\n<p> accelerate==0.24.1\n<p> optimum==1.14.1\n<p> auto-gptq==0.5.1+cu118\n<p> safetensors==0.4.0\n### Other Requirements\n<p> PostgreSQL database"
        },
        {
            "comment": "This code snippet explains the language model used in the project, which is Zephyr-7b-beta, and provides information about its quantized version due to VRAM requirements. It also mentions using all-mpnet-base-v2 for generating embeddings, details on the OS and hardware used (Windows 10 and Nvidia GTX 1660Ti Max-Q Design), and addresses the problem of hallucination in LLMs by introducing Retrieval Augmented approach.",
            "location": "\"/media/root/Prima/works/local_rag/docs/src/README.md\":56-85",
            "content": "<p> pgvector extension\n<p> CUDA 11.8\n## Language Model\nI used Zephyr-7b-beta in this project. It is state-of-the-art language model\ndeveloped by HuggingfaceH4 on top of the Mistral-7b model.\nBecause of extensive VRAM requirements of the non-quantized model, I used its 4-bit\nquantized version.\nYou can find the model here: https://huggingface.co/TheBloke/zephyr-7B-beta-GPTQ\nFor generating embeddings, I used all-mpnet-base-v2. It is one of the most commonly\nmodels.\nYou can find the model here: https://huggingface.co/sentence-transformers/all-mpnet-base-v2\n## OS & Hardware\nI am currently using Windows 10. If you are using different OS, download & install\nthe packages for your needs.\nI have Nvidia GTX 1660Ti Max-Q Design 6 GB on my laptop and quantized model fits in\nmy GPU, takes up to 5.7 GB of VRAM. If you don't have enough VRAM, you can try\ndifferent model from TheBloke's quantized models that can run on CPU.\n# Talk with Your PDFs\nOne of the main problems with LLMs is that they hallucinate. Retrieval Augmented"
        },
        {
            "comment": "The code introduces RAG algorithm, explaining its purpose to provide ground truth information for LLMs and decrease hallucination rates. It suggests using PDF files for analysis and summarization tasks, mentioning possible applications in CVs, news, and homework articles. The code also encourages users to design a UI with Flask or dockerize the application for local usage, ensuring privacy of documents.",
            "location": "\"/media/root/Prima/works/local_rag/docs/src/README.md\":86-103",
            "content": "Generation (RAG) is an algorithm developed by Meta AI researchers. The main idea\nbehind RAG is providing ground truth information to the LLM and let it generate\nits responses based on this information. The source of the information can be\ndocuments, can be databases, datasets or the internet. The responses generated\nby the language model anchored to the facts better and decreases its hallucination\nrates. Also, same approach can be used in analyzing, summarizing or deriving\ninformation using your documents.\nPut your PDF files in a folder and talk with the model. I provided some example\nacademic articles. You can use this approach in analyzing and summarizing CVs,\nnews, etc. And that's it. :)\nYou can design UI with web frameworks like Flask and even dockerize it. Now, you\nhave your own local language model that always up. You don't have to share your\nprivate documents with 3rd parties to get the summarization. You can even use\nthe model to write your homework articles. :) Shhh\nEnjoy!"
        }
    ]
}